# LLM Engineering & Optimization Lab

–í —ç—Ç–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ —Å–æ–±—Ä–∞–Ω—ã R&D –ø—Ä–æ–µ–∫—Ç—ã, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM): –æ—Ç –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è (Fine-Tuning) –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (Alignment).

–ê–≤—Ç–æ—Ä: –î–∞–Ω–∏–∏–ª –ü—É—à–∫–∞—Ä—å
–†–æ–ª—å: Product Lead | –ê—Å–ø–∏—Ä–∞–Ω—Ç –ò–¢–ú–û
–§–æ–∫—É—Å: –°–æ–µ–¥–∏–Ω—è—é –≥–ª—É–±–æ–∫—É—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É (R&D) —Å –±–∏–∑–Ω–µ—Å-—Ü–µ–Ω–Ω–æ—Å—Ç—å—é –≤ AI-–ø—Ä–æ–¥—É–∫—Ç–∞—Ö.

---

## üõ† –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤ (Projects Overview)

### 1. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏ BPE –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è(./01-transformer-architecture/transformer-bpe-impl.ipynb)
–î–æ–º–µ–Ω: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ NLP Core
–ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ ¬´–¥–≤–∏–∂–æ–∫¬ª —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ NLP. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å –Ω—É–ª—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.
 –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: PyTorch, Custom BPE Tokenizer.
 –†–µ–∑—É–ª—å—Ç–∞—Ç: –û–±—É—á–∏–ª –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è (–Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –∞–Ω–µ–∫–¥–æ—Ç–æ–≤), –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–≤ –≤–ª–∏—è–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞.

### 2. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ Alignment: DPO vs PPO(./02-llm-alignment-research/dpo-vs-ppo-alignment.ipynb)
–î–æ–º–µ–Ω: AI Safety & Alignment
–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è LLM —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.
 –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: TRL (Transformer Reinforcement Learning), PyTorch.
 –†–µ–∑—É–ª—å—Ç–∞—Ç: –ü—Ä–æ–≤–µ–ª —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ RLHF (PPO) –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ DPO. –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ DPO –≤ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, —á—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –≤ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ–¥—É–∫—Ç—ã (—Å–Ω–∏–∂–µ–Ω–∏–µ TCO).

### 3. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ (PEFT/LoRA)(./03-peft-optimization/lora-dora-finetuning.ipynb)
–î–æ–º–µ–Ω: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ (Resource Optimization)
–î–æ–æ–±—É—á–µ–Ω–∏–µ —Ç—è–∂–µ–ª—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –∂–µ–ª–µ–∑–µ (Consumer-grade GPU) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞.
 –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: Hugging Face PEFT, BitsAndBytes (Quantization).
 –†–µ–∑—É–ª—å—Ç–∞—Ç: –£—Å–ø–µ—à–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–ª LLM –ø–æ–¥ –¥–æ–º–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É (–∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏), –∏—Å–ø–æ–ª—å–∑—É—è –º–µ–Ω–µ–µ 10% –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü–æ–¥—Ç–≤–µ—Ä–¥–∏–ª –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å On-premise —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ (T4 GPU).

### 4. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ú–∏–∫—Ä–æ–≥—Ä–∏–¥–æ–º –∏ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¢–∞—Ä–∏—Ñ–æ–≤ (Gradient Boosting)(./04-microgrid-optimization)
–î–æ–º–µ–Ω: Predictive Analytics & Energy Tech
–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ–º –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è (Microgrid). –ó–∞–¥–∞—á–∞: –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è –∑–∞—Ç—Ä–∞—Ç –Ω–∞ —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏—é –∑–∞ —Å—á–µ—Ç —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–∞–º–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–∏ (ESS).
–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: CatBoost (Gradient Boosting), Pandas, Time Series Feature Engineering (Lags, Rolling windows).
–ö–ª—é—á–µ–≤—ã–µ –º–æ–¥—É–ª–∏:
     Forecasting Engine: –ú–æ–¥–µ–ª—å –Ω–∞ –±–∞–∑–µ CatBoostRegressor –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—á–∞—Å–æ–≤–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è (—Å —É—á–µ—Ç–æ–º —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏, –ª–∞–≥–æ–≤ –∏ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).
     Economic Optimization: –ê–ª–≥–æ—Ä–∏—Ç–º –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ü–µ–Ω–æ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏–∏ (1-4 —Ü–µ–Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏) –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥–Ω–æ–∑–∞.
     Storage Control: –õ–æ–≥–∏–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞—Ä—è–¥–æ–º/—Ä–∞–∑—Ä—è–¥–æ–º –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—è (`regulation_algorithm.py`) –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –ø–∏–∫–æ–≤ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è (Peak Shaving).
 –†–µ–∑—É–ª—å—Ç–∞—Ç: –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω –ø–∞–π–ø–ª–∞–π–Ω, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É (MAE/RMSE), –Ω–æ –∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç –æ—Ç –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ç–∞—Ä–∏—Ñ–æ–≤ –∏ —Ä–∞–±–æ—Ç—ã –±–∞—Ç–∞—Ä–µ–∏. –ü—Ä–æ–≤–µ–¥–µ–Ω –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö (Fine-tuning) –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ —Ç–∞—Ä–∏—Ñ–∞.


## üíª –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫
 Frameworks: PyTorch, Hugging Face (Transformers, PEFT, TRL).
 Optimization: BitsAndBytes (4-bit/8-bit quantization), LoRA/DoRA.
 Environment: Google Colab (T4 GPU), Linux.
